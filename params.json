{
  "name": "Bitwise-nn-fpga",
  "tagline": "",
  "body": "# FPGA Implementation of Bitwise Neural Networks\r\nTrace Russell\r\nCarnegie Mellon University, 15418 Spring 2016 Final Project\r\n\r\n## Final Writeup\r\nI developed an FPGA implementation of bitwise neural networks, where all of the arithmetic operations consist solely of bitwise operations and integer addition/comparison. I explore several different optimizations for adding pipelining and parallelism to the hardware to get speedups over the baseline sequential implementation. I did not have time to get my implementation running on actual hardware, but I give theoretical calculations showing how my implementation would get several orders of magnitude speedup over off-the-shelf non-bitwise neural network implementations for CPUs and GPUs, primarily due to more efficient use of memory bandwidth.\r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. Recently, bitwise networks have been shown to give accuracy results competitive with non-bitwise networks ([1], [2]). A forward pass through the network consists only of bitwise operations, plus a few integer additions and comparisons. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nFor a fully-connected neural network for classification tasks, the input is a binary vector, and the output is an N-dimensional vector, where N is the number of classes. Each non-output layer of the network consists of \"multiplying\" the input vector by a weight matrix (the binary multiplication is actually just XNOR), and then comparing the integer values of the output vector with a threshold to convert them back into binary values. The output layer of the network skips the thresholding step, and instead the class with the largest integer value is the predicted class. The basics of computing a forward pass through the network can be summarized as follows:\r\n\r\n    for each layer:\r\n        for each output neuron:\r\n            acc = 0\r\n            for each input neuron:\r\n                acc += weight XNOR input\r\n            output = threshold(acc)\r\n\r\nThe outermost 'layer' loop is not parallelizable, because each layer depends on the results of the previous layer. However, either of the innermost loops is trivially parallelizable. Because there is only about 1 mathematical operation per weight, it turns out that memory bandwidth becomes the limiting factor for getting speedup improvements. As a result, for a hardware implementation most of the speedup is achieved through pipelining the innermost loops. I discuss this in more detail later, but a key takeaway is that most of the parallelism in a hardware bitwise neural network implementation comes in the form of pipelining. The goal is to achieve a total throughput of 1 operation per cycle, as this will saturate the memory bus and any further speedups are impossible.\r\n\r\n### Approach\r\nI first started by using open source bitwise neural network code [1] to train a network from scratch. I made some modifications to the code, such as removing a few non-bitwise operations from the network and simplifying batch normalization layers. I then wrote some Lua scripts (available in this repository) to extract the binary weights and inputs, and package them in a format suitable for loading from C code. I used the MNIST handwritten digits dataset, and the network architecture consists of 4 fully-connected layers. The sizes of the layers are 784-2048-2048-2048-10. The trained bitwise neural network achieves a test set accuracy of about 94%.\r\n\r\n![](https://raw.githubusercontent.com/tdrussell/bitwise-nn-fpga/master/img/training.png)\r\n\r\nI implemented the main computational kernel using Xilinx's Vivado High-Level Synthesis (HLS), which which takes a sequential C or C++ description of an algorithm and synthesizes it into RTL Verilog or VHDL. HLS automatically parallelizes any operation it can. A good example of this is the following code:\r\n\r\n    u32 countBits(u32 x)\r\n    {\r\n        u32 count = 0;\r\n        for (int i = 0; i < 32; i++) {\r\n            count += x & 1;\r\n            x >>= 1;\r\n        }\r\n        return count;\r\n    }\r\n\r\nA naive hardware synthesis of this code would take 32 cycles to complete. However, HLS is able to automatically infer a combinatorial logarithmic adder tree implementation, which only takes 1 cycle to complete. In addition to automatic parallelization, HLS lets the programmer specify various synthesis directives such as array partitioning and loop pipelining to guide the synthesis process and expose more parallelism.\r\n\r\nMy baseline implementation is a sequential version of the algorithm with no pipelining. The only parallelism present is what HLS is able to infer without any added synthesis directives. This version takes ~11,000,000 cycles to process one input image and predict a classification. The design uses 11% of the lookup tables (LUTs) for my FPGA board.\r\n\r\nThe first optimization I make is to add pipelining to the inner loop (over input neurons). The new version takes ~627,000 cycles to process one image. The non-output layers have an initiation interval (II) of 2, which means they are pipelined and start processing a new set of 32 weights every 2 clock cycles. This design uses 16% of the LUTs, a moderate increases over the sequential version due to the pipelining, which must replicate hardware.\r\n\r\nThe second optimization is to change the memory access patterns to get the II of the inner loops down to 1, which means the entire design has a throughput of about one set of 32 weights per clock cycle, which is the target we are aiming for. The previous design packed the output of a layer as follows:\r\n\r\n    u32 mask = 0x1 << shift;\r\n    scratch[1][j/32] = (scratch[1][j/32] & ~mask) | (out << shift);\r\n\r\nThe second line is a read-modify-write operation on scratch memory residing in block RAM, which will always take the hardware 2 cycles to complete. As a results, the maximum II of the inner loop is limited to 2. I change this code to the following:\r\n\r\n    tmp = tmp | (out << shift);\r\n    if (j % 32 == 31) {\r\n        scratch[1][j/32] = tmp;\r\n        tmp = 0;\r\n    }\r\n\r\nThe bits are now packed into a temporary register, which is then written to memory and cleared when it fills up. There is now only a memory write, and the II of the new design drops to 1 as expected. This new version now takes ~357,000 cycles to process one image.\r\n\r\nThe third optimization is to pipeline the outer loop (over output neurons). In HLS, pipelining outer loops creates significantly more hardware replication, but can give higher throughputs. In my case, the hardware for the entire inner loop is replicated multiple times. One interesting thing that HLS does is create 2 memory ports on my design (there are multiple physical ports connecting the FPGA logic to the memory bus). The loop hardware is replicated such that there is actually more than 1 read per clock cycle, and multiple sets of weights are processed in parallel in addition to the pipelining. This final version takes ~162,000 cycles to process one image, but takes up 87% of the LUTs on my FPGA board and likely is not realizable in the actual hardware.\r\n\r\n### Results\r\n\r\n![](https://raw.githubusercontent.com/tdrussell/bitwise-nn-fpga/master/img/speedup.png)\r\n\r\nAlthough I did not have time to get my design to run on actual hardware, I will now give calculations for theoretical speedup and power efficiency. Deep learning tasks are known to be memory bandwidth bound [3], especially when the arithmetic intensity is low due to lack of convolutional layers, which is the case in my project. My target platform is a Zybo Zynq development board, which has a memory bandwidth of 1050 Mbps. Assuming 90% saturation of memory bandwidth, or 118.125 MB/s, even the Optimization #1 version of my implementation becomes restricted by the memory bandwidth. There are about 1.3 MB of weights that need to be read for processing each image, which gives a theoretical result of 0.011s per input image on the FPGA hardware.\r\n\r\nI measured CPU and GPU execution time of the same neural network architecture that I implemented, the only difference being that the CPU and GPU do not pack the binary weights densely. Both CPU and GPU implementations use off-the-shelf Torch modules with no additional optimizations. A i7-3770K CPU takes on average 0.0113s per image, and a GTX980 GPU takes 0.0000825s per image. The i7-3770K has a TDP of 77W, and the GTX980 165W. I used Xilinx's Power Estimator tool and got a power usage of 1.9W for my Optimization #1 design. All of these numbers give the following graph for energy used per image:\r\n\r\n![](https://raw.githubusercontent.com/tdrussell/bitwise-nn-fpga/master/img/efficiency.png)\r\n\r\nMy low-end FPGA development board is much more efficient than a CPU neural network implementation, and barely worse off than the GPU. The GPU beats the FPGA mainly due to its massive memory bandwidth of 224 GB/s, which lets it excel at bandwidth-bound tasks. In terms of total power consumption, the FPGA is by far the lowest with 1.9W, suitable for even mobile applications. These results indicate that an FPGA implementation of bitwise neural networks excels in situations that demand both decent performance and low total power consumption.\r\n\r\n### References:\r\n[1] Courbariaux, Matthieu, Hubara, Itay, Soudry, Daniel, El-Yaniv, Ran, and Bengio, Yoshua. Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1. http://arxiv.org/pdf/1602.02830.pdf\r\n\r\n[2] Kim, Minje and Smaragdis, Paris. Bitwise Neural Networks. http://arxiv.org/pdf/1601.06071v1.pdf\r\n\r\n[3] Deng, Zhaoxia, Xu, Cong, Cai, Qiong, and Faraboschi, Paolo. Reduced-Precision Memory Value Approximation for Deep Learning. http://www.labs.hpe.com/techreports/2015/HPL-2015-100.pdf\r\n\r\n\r\n\r\n## Archive\r\nThis is the original proposal and midway report. Information may be out of date. For reference purposes.\r\n\r\n## Project Proposal\r\nThis project will consist of an FPGA-based implementation of bitwise neural networks. I currently plan on implementing the system on a Xilinx Zynq-7000 FPGA. \r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. A forward pass through the network consists only of bitwise operations and bit counting. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nThe FPGA hardware naturally gives many opportunities to exploit parallelism in order to speed up forward passes through the neural network. For instance, an obvious parallelization opportunity is to compute an single neuron's weight 'multiplications' (actually bitwise operations) in parallel. There may also be chances to compute multiple neuron activations in parallel. Furthermore, adding up the results of weight multiplications might be done with a parallel logarithmic adder tree.\r\n\r\n### The Challenge:\r\nThis project has many key challenges. The main overarching challenge is how to efficiently represent the bitwise neural network on the FPGA, and how to maximally exploits parallelism inherent in the FPGA hardware resources. For the network representation, the key point is where the weights will be stored. For small networks, it might be possible to store the network weights in block RAM on the FPGA itself. Larger networks might require storing the weights in main system RAM, and streaming them in during a forward pass along with the input data itself. Both of these options will need to be explored.\r\n\r\nThere are many avenues for parallelization. These include parallelizing weight multiplications for a single neuron, and computing the activations of possibly more than one neuron in parallel. There are also synchronization challenges, because computations for one layer of the network can only begin once the activations of the previous layer are fully computed. In short, the parallelization and synchronization challenges in this project are nontrivial and will require significant research and investigation to arrive at an optimal implementation.\r\n\r\nThere are many constraints inherent in the FPGA hardware. There are are limited amounts of LUTs and block ram units. Perhaps more importantly for neural networks in particular, logic routing may become a limiting factor due to the very high connectivity between layers of the network. It is almost certainly impossible to literally wire up one fully-connected layer to another, so I will need to come up with some type of serialization strategy to process only one part of the network at a time.\r\n\r\n### Resources:\r\nAs mentioned above, I currently plan on using a Xilinx Zynq-7000 based FPGA development board for the implementation, though this might change as I more thoroughly research hardware options. The Zync SoC has an integrated ARM processor, and I believe this can make the implementation simpler in some ways. For instance, I can use the embedded Linux running on the ARM processor to manage allocating memory in main RAM, and offload a forward pass of an example through the network to the FPGA.\r\n\r\nThe main goal of this project is implementing an FPGA architecture for running bitwise neural networks, rather than the design of software that allows training those bitwise networks. Luckily, the authors of [1] have released their research software that allows training and running binarized neural networks on CPUs and GPUs. One of their implementations is for the Torch7 scientific computing framework, which I am already well familiar with. As such, it should not be difficult for me to use this software to train a bitwise neural network whose implementation I will develop on the FPGA. The paper of [2] is also another look into bitwise neural networks, and it may contain useful information to guide me as well.\r\n\r\n15418 is primarily a computer science course, and this project heavily involves an FPGA, so I feel it is worth mentioning that I am a CS master's student focusing on machine learning, but previously I did my undergraduate degree in electrical engineering. Specifically, I focused on integrated circuit design, and had the opportunity to write both Verilog and VHDL for FPGAs in multiple courses. I don't believe the learning curve of switching from software to a hardware description language will too difficult for me. Furthermore, I think my knowledge of both machine learning and FPGA development makes me well-suited to tackling this project.\r\n\r\n### Goals and Deliverables:\r\nThe main goal I plan to achieve during this project is straightforward: a simple FPGA-based implementation of bitwise neural networks consisting of fully connected hidden layers. This type of network is the simplest, and is the main target I will aim for. For the training and testing dataset I will use MNIST, which consists of handwritten digits.\r\n\r\nIf the project goes very well, I would additionally like to achieve an implementation of convolutional bitwise neural networks. Convolutional networks are far more useful for image recognition tasks than simple networks of fully-connected layers, which would expand the application scope of my FPGA-based implementation.\r\n\r\nAt the parallelism competition, I will hopefully show a demo of my implementation classifying the entire MNIST testing dataset in a short amount of time (on a GPU this takes a few seconds, and hopefully my implementation will be competitive). I will also present graphs characterizing the performance (running time) and classification accuracy of different bitwise network models. I will compare these numbers with the full network models running on both a CPU and GPU (this is easy to do with Torch).\r\n\r\nIt is hard to predict what kind of exact performance numbers I am expecting from my implementation. The most I can say is that I hope to be within an order of magnitude of the runtime of the full neural network executing on a GPU. Even if the bitwise network running on an FPGA is much slower compared to the full network running on the GPU, the power efficiency will be much higher and I can give calculations for those numbers as well.\r\n\r\n### Platform Choice:\r\nAs mentioned above, I currently plan on using a Zynq-7000 based development board. The Zynq SoC has reprogrammable logic with 10s of thousands of LUTs, and hundreds of KB of block ram. In addition there is an ARM processor capable of running embedded Linux and interfacing with the reconfigurable logic portion of the SoC.\r\n\r\n### Schedule:\r\n(Listed week-by-week, labelled by the Sunday that starts the week)\r\n\r\n4/3: Finalize selection of hardware, and acquire said hardware. Talk around on campus and see if I can borrow an appropriate FPGA dev board for free. If not, I can purchase something like a Zedboard or Parallella, which is within my budget I am willing to spend. (complete)\r\n\r\n4/10 (week of checkpoint): Create initial sanity-check Verilog code for the FPGA that does something simple like pull data out of RAM, add something to it, and write the result back to RAM. This will give me the foundations of a input-processing-output pipeline, which is essentially what a feedforward pass through the neural network is. Use the software from [1] to train my own bitwise neural network on a dataset like MNIST. I have a GTX 980 GPU, and can spare a few hours or even a full day on my own computer to train the model. Also start sketching out architectural ideas for the FPGA implementation. (complete)\r\n\r\n4/17: Begin designing the architecture for the bitwise neural network implementation. This will involve writing Verilog or using Vivado HLS. (in progress / partially complete)\r\n\r\n4/24: Start putting all the pieces of the architecture together. Create an example application that can pass a testing example through the bitwise neural network on the FPGA and produce a classification prediction. (in progress / partially complete)\r\n\r\n5/1: Begin working on the final writeup and project presentation. If things have been going very well and are ahead of schedule, investigate the feasibility of implementing a convolutional bitwise neural network on the FPGA in addition to a network of fully-connected layers.\r\n\r\n5/8: Final preparations for the presentation on Monday, and final adjustments to the writeup that is also due Monday.\r\n\r\n### Checkpoint\r\nSo far I have gathered all necessary materials, trained a bitwise neural network, and done some preliminary coding of the FPGA neural network architecture. I bought a Zybo development board which has a Zynq-7000 FPGA. I also downloaded Xilinx Vivado and confirmed that some test applications synthesize to RTL and pass the testbenches correctly. I used the code provided in [1] to train a bitwise neural network for the MNIST dataset. The network is composed of several fully connected binary layers. I also wrote some Lua scripts to export the network weights and MNIST images into suitable binary files that can be loaded from C code.\r\n\r\nFor the FPGA bitwise neural network architecture, I decided to use Vivado high-level synthesis (HLS), which allows you to write C code that can also be synthesized to RTL that can be loaded onto the FPGA. This is not as efficient as writing Verilog by hand, but it let me quickly get a baseline solution running. My current code is a mostly-complete baseline implementation, but currently does not give correct classification results for the MNIST images. There is probably a bug somewhere that I hope to solve within the next few days. Regardless, all of the code is in place for loading the network weights and performing the bitwise operations to predict a label, so I am mostly fine with the results so far.\r\n\r\nOnce I fix the current bugs, the next step will be to synthesize the code to RTL and start trying to improve performance. There are many ways to improve performance in Vivado HLS by pipelining operations and using parallelism. Xilinx has provided HLS tutorials that I can use to try out different performance enhancements. I may also rewrite some or all of my HLS code in Verilog, which takes more effort but would give me more control of how the hardware synthesizes. The final step will be to test out the code on the actual FPGA hardware.\r\n\r\nI have completed the first part of my goals as listed in the original schedule. I am currently on track to be able to complete the main deliverables and produce a working design by the time of the Parallelism competition. At this point I don't know whether I will be able to complete the optimistic goal of a convolutional bitwise neural network. It depends on how long it takes me to iron out the current bugs in my code and how well the performance optimizations go. For the parallelism competition I still plan to show a demo of my code running the classification of the MNIST images on the FPGA hardware. I will also show graphs and charts characterizing the performance and power consumption, and comparing it to that of a CPU and GPU.\r\n\r\nI have only two main concerning issues at this point. The first is the correctness bugs in my current code. The code runs in simulation and produces classification predictions, but they do not seem to be correct. Hopefully it will only take me a few days at most to track down the source of this issue and get it resolved. The second main concern is whether I can get significant performance improvements using only Vivado HLS, without having to manually write Verilog. I am still new to HLS, but using it so far has made developing the code much faster than it would have taken me if I wrote it in Verilog. The ideal scenario is to get reasonable performance out of the FPGA while still writing high-level HLS code, but I am still unsure of how possible this is.\r\n\r\nHere is a new detailed schedule for the second part of the project. It is broken down by roughly half-week increments. The dates are the planned deadlines for the listed goals.\r\n\r\n4/21: Fix the correctness bugs in the baseline implementation. Verify that the RTL synthesis of the current code works correctly.\r\n\r\n4/25: Improve performance over the baseline implementation by using pipelining and parallel bitwise computations.\r\n\r\n4/29: Make any necessary modifications to the code so that it can be run as an application on the FPGA hardware.\r\n\r\n5/3: If time, get more performance improvements by rewriting some or all of the main computational kernel in Verilog. Possibly implement convolutional bitwise networks.\r\n\r\n5/8: Finalize website and create final report.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}