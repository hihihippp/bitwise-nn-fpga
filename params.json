{
  "name": "Bitwise-nn-fpga",
  "tagline": "",
  "body": "# FPGA Implementation of Bitwise Neural Networks\r\nTrace Russell\r\n\r\n## Project Proposal\r\nThis project will consist of an FPGA-based implementation of bitwise neural networks. I currently plan on implementing the system on a Xilinx Zynq-7000 FPGA. \r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. A forward pass through the network consists only of bitwise operations and bit counting. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nThe FPGA hardware naturally gives many opportunities to exploit parallelism in order to speed up forward passes through the neural network. For instance, and obvious parallelization opportunity is to compute an single neuron's weight 'multiplications' (actually bitwise operations) in parallel. There may also be chances to compute multiple neuron activations in parallel. Furthermore, adding up the results of weight multiplications might be done with a parallel logarithmic adder tree.\r\n\r\n### The Challenge:\r\nThis project has many key challenges. The main overarching challenge is how to efficiently represent the bitwise neural network on the FPGA. The key point here is where the weights will be stored. For small networks, it might be possible to store the network weights in block RAM on the FPGA itself. Larger networks might require storing the weights in main system RAM, and streaming them in during a forward pass along with the input data itself. Both of these options will need to be explored.\r\n\r\n### Resources:\r\nAs mentioned above, I currently plan on using a Xilinx Zynq-7000 based FPGA development board for the implementation, though this might change as I more thoroughly research hardware options. The Zync SoC has an integrated ARM processor, and I believe this can make the implementation simpler in some ways. For instance, I can use the embedded Linux running on the ARM processor to manage allocating memory in main RAM, and offload a forward pass of an example through the network to the FPGA.\r\n\r\nThe main goal of this project is implementing an FPGA architecture for running bitwise neural networks, rather than the design of software that allows training those bitwise networks. Luckily, the authors of [1] have released their research software that allows training and running binarized neural networks on CPUs and GPUs. One of their implementations is for the Torch7 scientific computing framework, which I am already well familiar with. As such, it should not be difficult for me to use this software to train a bitwise neural network whose implementation I will develop on the FPGA.\r\n\r\n### Goals and Deliverables\r\nTo be added.\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}