{
  "name": "Bitwise-nn-fpga",
  "tagline": "",
  "body": "# FPGA Implementation of Bitwise Neural Networks\r\nTrace Russell\r\nCarnegie Mellon University, 15418 Spring 2016 Final Project\r\n\r\n## Final Writeup\r\nI developed an FPGA implementation of bitwise neural networks, where all of the arithmetic operations consist solely of bitwise operations and integer addition/comparison. I explore several different optimizations for adding pipelining and parallelism to the hardware to get speedups over the baseline sequential implementation. I did not have time to get my implementation running on actual hardware, but I give theoretical calculations showing how my implementation would get several orders of magnitude speedup over off-the-shelf non-bitwise neural network implementations for CPUs and GPUs, primarily due to more efficient use of memory bandwidth.\r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. A forward pass through the network consists only of bitwise operations, plus a few integer additions and comparisons. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nFor a fully-connected neural network for classification tasks, the input is a binary vector, and the output is an N-dimensional vector, where N is the number of classes. Each non-output layer of the network consists of \"multiplying\" the input vector by a weight matrix (the binary multiplication is actually just XNOR), and then comparing the integer values of the output vector with a threshold to convert them back into binary values. The output layer of the network skips the thresholding step, and instead the class with the largest integer value is the predicted class. The basics of computing a forward pass through the network can be summarized as follows:\r\n\r\nfor each layer:\r\n    for each output neuron:\r\n        acc = 0\r\n        for each input neuron:\r\n            acc += weight XNOR input\r\n        output = threshold(acc)\r\n\r\nThe outermost 'layer' loop is not parallelizable, because each layer depends on the results of the previous layer. However, either of the innermost loops is trivially parallelizable. Because there is only about 1 mathematical operation per weight, it turns out that memory bandwidth becomes the limiting factor for getting speedup improvements. As a result, for a hardware implementation most of the speedup is achieved through pipelining the innermost loops. I discuss this in more detail later, but a key takeaway is that most of the parallelism in a hardware bitwise neural network implementation comes in the form of pipelining. The goal is to achieve a total throughput of 1 operation per cycle, as this will saturate the memory bus and any further speedups are impossible.\r\n\r\n### Approach\r\nI first started by using open source bitwise neural network code [1] to train a network from scratch. I made some modifications to the code, such as removing a few non-bitwise operations from the network and simplifying batch normalization layers. I then wrote some Lua scripts (available in this repository) to extract the binary weights and inputs, and package them in a format suitable for loading from C code.\r\n\r\nI implemented the main computational kernel using Xilinx's Vivado High-Level Synthesis (HLS), which which takes a sequential C or C++ description of an algorithm and synthesizes it into RTL Verilog or VHDL. HLS automatically parallelizes any operation it can. A good example of this is the following code:\r\n\r\n(insert code here)\r\n\r\nA naive hardware synthesis of this code would take 32 cycles to complete. However, HLS is able to automatically infer a combinatorial logarithmic adder tree implementation, which only takes 1 cycle to complete. In addition to automatic parallelization, HLS lets the programmer specify various synthesis directives such as array partitioning and loop pipelining to guide the synthesis process and expose more parallelism.\r\n\r\n### References:\r\n[1] Binarized Neural Networks: Training Neural Networks with Weights and\r\nActivations Constrained to +1 or -1. (http://arxiv.org/pdf/1602.02830.pdf)\r\n\r\n[2] Bitwise Neural Networks. (http://arxiv.org/pdf/1601.06071v1.pdf)\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}