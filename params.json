{
  "name": "Bitwise-nn-fpga",
  "tagline": "",
  "body": "# FPGA Implementation of Bitwise Neural Networks\r\nTrace Russell\r\n\r\n## Project Proposal\r\nThis project will consist of an FPGA-based implementation of bitwise neural networks. I currently plan on implementing the system on a Xilinx Zynq-7000 FPGA. \r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. A forward pass through the network consists only of bitwise operations and bit counting. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nThe FPGA hardware naturally gives many opportunities to exploit parallelism in order to speed up forward passes through the neural network. For instance, an obvious parallelization opportunity is to compute an single neuron's weight 'multiplications' (actually bitwise operations) in parallel. There may also be chances to compute multiple neuron activations in parallel. Furthermore, adding up the results of weight multiplications might be done with a parallel logarithmic adder tree.\r\n\r\n### The Challenge:\r\nThis project has many key challenges. The main overarching challenge is how to efficiently represent the bitwise neural network on the FPGA, and how to maximally exploits parallelism inherent in the FPGA hardware resources. For the network representation, the key point is where the weights will be stored. For small networks, it might be possible to store the network weights in block RAM on the FPGA itself. Larger networks might require storing the weights in main system RAM, and streaming them in during a forward pass along with the input data itself. Both of these options will need to be explored.\r\n\r\nThere are many avenues for parallelization. These include parallelizing weight multiplications for a single neuron, and computing the activations of possibly more than one neuron in parallel. There are also synchronization challenges, because computations for one layer of the network can only begin once the activations of the previous layer are fully computed. In short, the parallelization and synchronization challenges in this project are nontrivial and will require significant research and investigation to arrive at an optimal implementation.\r\n\r\nThere are many constraints inherent in the FPGA hardware. There are are limited amounts of LUTs and block ram units. Perhaps more importantly for neural networks in particular, logic routing may become a limiting factor due to the very high connectivity between layers of the network. It is almost certainly impossible to literally wire up one fully-connected layer to another, so I will need to come up with some type of serialization strategy to process only one part of the network at a time.\r\n\r\n### Resources:\r\nAs mentioned above, I currently plan on using a Xilinx Zynq-7000 based FPGA development board for the implementation, though this might change as I more thoroughly research hardware options. The Zync SoC has an integrated ARM processor, and I believe this can make the implementation simpler in some ways. For instance, I can use the embedded Linux running on the ARM processor to manage allocating memory in main RAM, and offload a forward pass of an example through the network to the FPGA.\r\n\r\nThe main goal of this project is implementing an FPGA architecture for running bitwise neural networks, rather than the design of software that allows training those bitwise networks. Luckily, the authors of [1] have released their research software that allows training and running binarized neural networks on CPUs and GPUs. One of their implementations is for the Torch7 scientific computing framework, which I am already well familiar with. As such, it should not be difficult for me to use this software to train a bitwise neural network whose implementation I will develop on the FPGA. The paper of [2] is also another look into bitwise neural networks, and it may contain useful information to guide me as well.\r\n\r\n15418 is primarily a computer science course, and this project heavily involves an FPGA, so I feel it is worth mentioning that I am a CS master's student focusing on machine learning, but previously I did my undergraduate degree in electrical engineering. Specifically, I focused on integrated circuit design, and had the opportunity to write both Verilog and VHDL for FPGAs in multiple courses. I don't believe the learning curve of switching from software to a hardware description language will too difficult for me. Furthermore, I think my knowledge of both machine learning and FPGA development makes me well-suited to tackling this project.\r\n\r\n### Goals and Deliverables:\r\nThe main goal I plan to achieve during this project is straightforward: a simple FPGA-based implementation of bitwise neural networks consisting of fully connected hidden layers. This type of network is the simplest, and is the main target I will aim for. For the training and testing dataset I will use MNIST, which consists of handwritten digits.\r\n\r\nIf the project goes very well, I would additionally like to achieve an implementation of convolutional bitwise neural networks. Convolutional networks are far more useful for image recognition tasks than simple networks of fully-connected layers, which would expand the application scope of my FPGA-based implementation.\r\n\r\nAt the parallelism competition, I will hopefully show a demo of my implementation classifying the entire MNIST testing dataset in a short amount of time (on a GPU this takes a few seconds, and hopefully my implementation will be competitive). I will also present graphs characterizing the performance (running time) and classification accuracy of different bitwise network models. I will compare these numbers with the full network models running on both a CPU and GPU (this is easy to do with Torch).\r\n\r\nIt is hard to predict what kind of exact performance numbers I am expecting from my implementation. The most I can say is that I hope to be within an order of magnitude of the runtime of the full neural network executing on a GPU. Even if the bitwise network running on an FPGA is much slower compared to the full network running on the GPU, the power efficiency will be much higher and I can give calculations for those numbers as well.\r\n\r\n### Platform Choice:\r\nAs mentioned above, I currently plan on using a Zynq-7000 based development board. The Zynq SoC has reprogrammable logic with 10s of thousands of LUTs, and hundreds of KB of block ram. In addition there is an ARM processor capable of running embedded Linux and interfacing with the reconfigurable logic portion of the SoC.\r\n\r\n### Schedule:\r\n(Listed week-by-week, labelled by the Sunday that starts the week)\r\n4/3: Finalize selection of hardware, and acquire said hardware. Talk around on campus and see if I can borrow an appropriate FPGA dev board for free. If not, I can purchase something like a Zedboard or Parallella, which is within my budget I am willing to spend.\r\n\r\n4/10 (week of checkpoint): Create initial sanity-check Verilog code for the FPGA that does something simple like pull data out of RAM, add something to it, and write the result back to RAM. This will give me the foundations of a input-processing-output pipeline, which is essentially what a feedforward pass through the neural network is. Use the software from [1] to train my own bitwise neural network on a dataset like MNIST. I have a GTX 980 GPU, and can spare a few hours or even a full day on my own computer to train the model. Also start sketching out architectural ideas for the FPGA implementation.\r\n\r\n4/17: Begin designing the architecture for the bitwise neural network implementation. This will involve writing Verilog, and very likely using a logic simulator to start testing individual modules.\r\n\r\n4/24: Start putting all the pieces of the architecture together. Create an example application that can pass a testing example through the bitwise neural network on the FPGA and produce a classification prediction.\r\n\r\n5/1: Begin working on the final writeup and project presentation. If things have been going very well and are ahead of schedule, investigate the feasibility of implementing a convolutional bitwise neural network on the FPGA in addition to a network of fully-connected layers.\r\n\r\n5/8: Final preparations for the presentation on Monday, and final adjustments to the writeup that is also due Monday.\r\n\r\n### References:\r\n[1] Binarized Neural Networks: Training Neural Networks with Weights and\r\nActivations Constrained to +1 or -1. (http://arxiv.org/pdf/1602.02830.pdf)\r\n\r\n[2] Bitwise Neural Networks. (http://arxiv.org/pdf/1601.06071v1.pdf)\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}