{
  "name": "Bitwise-nn-fpga",
  "tagline": "",
  "body": "# FPGA Implementation of Bitwise Neural Networks\r\nTrace Russell\r\nCarnegie Mellon University, 15418 Spring 2016 Final Project\r\n\r\n## Final Writeup\r\nI developed an FPGA implementation of bitwise neural networks, where all of the arithmetic operations consist solely of bitwise operations and integer addition/comparison. I explore several different optimizations for adding pipelining and parallelism to the hardware to get speedups over the baseline sequential implementation. I did not have time to get my implementation running on actual hardware, but I give theoretical calculations showing how my implementation would get several orders of magnitude speedup over off-the-shelf non-bitwise neural network implementations for CPUs and GPUs, primarily due to more efficient use of memory bandwidth.\r\n\r\n### Background:\r\nA bitwise neural network is a neural network where all the weights and activations are binary values. Recently, bitwise networks have been shown to give accuracy results competitive with non-bitwise networks ([1], [2]). A forward pass through the network consists only of bitwise operations, plus a few integer additions and comparisons. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.\r\n\r\nFor a fully-connected neural network for classification tasks, the input is a binary vector, and the output is an N-dimensional vector, where N is the number of classes. Each non-output layer of the network consists of \"multiplying\" the input vector by a weight matrix (the binary multiplication is actually just XNOR), and then comparing the integer values of the output vector with a threshold to convert them back into binary values. The output layer of the network skips the thresholding step, and instead the class with the largest integer value is the predicted class. The basics of computing a forward pass through the network can be summarized as follows:\r\n\r\n    for each layer:\r\n        for each output neuron:\r\n            acc = 0\r\n            for each input neuron:\r\n                acc += weight XNOR input\r\n            output = threshold(acc)\r\n\r\nThe outermost 'layer' loop is not parallelizable, because each layer depends on the results of the previous layer. However, either of the innermost loops is trivially parallelizable. Because there is only about 1 mathematical operation per weight, it turns out that memory bandwidth becomes the limiting factor for getting speedup improvements. As a result, for a hardware implementation most of the speedup is achieved through pipelining the innermost loops. I discuss this in more detail later, but a key takeaway is that most of the parallelism in a hardware bitwise neural network implementation comes in the form of pipelining. The goal is to achieve a total throughput of 1 operation per cycle, as this will saturate the memory bus and any further speedups are impossible.\r\n\r\n### Approach\r\nI first started by using open source bitwise neural network code [1] to train a network from scratch. I made some modifications to the code, such as removing a few non-bitwise operations from the network and simplifying batch normalization layers. I then wrote some Lua scripts (available in this repository) to extract the binary weights and inputs, and package them in a format suitable for loading from C code. I used the MNIST handwritten digits dataset, and the network architecture consists of 4 fully-connected layers. The sizes of the layers are 784-2048-2048-2048-10. The trained bitwise neural network achieves a test set accuracy of about 94%.\r\n\r\n![](https://raw.githubusercontent.com/tdrussell/bitwise-nn-fpga/master/img/training.png)\r\n\r\nI implemented the main computational kernel using Xilinx's Vivado High-Level Synthesis (HLS), which which takes a sequential C or C++ description of an algorithm and synthesizes it into RTL Verilog or VHDL. HLS automatically parallelizes any operation it can. A good example of this is the following code:\r\n\r\n(insert code here)\r\n\r\nA naive hardware synthesis of this code would take 32 cycles to complete. However, HLS is able to automatically infer a combinatorial logarithmic adder tree implementation, which only takes 1 cycle to complete. In addition to automatic parallelization, HLS lets the programmer specify various synthesis directives such as array partitioning and loop pipelining to guide the synthesis process and expose more parallelism.\r\n\r\nMy baseline implementation is a sequential version of the algorithm with no pipelining. The only parallelism present is what HLS is able to infer without any added synthesis directives. This version takes ~11,000,000 cycles to process one input image and predict a classification. The design uses 11% of the lookup tables (LUTs) for my FPGA board.\r\n\r\nThe first optimization I make is to add pipelining to the inner loop (over input neurons). The new version takes ~627,000 cycles to process one image. The non-output layers have an initiation interval (II) of 2, which means they are pipelined and start processing a new set of 32 weights every 2 clock cycles. This design uses 16% of the LUTs, a moderate increases over the sequential version due to the pipelining, which must replicate hardware.\r\n\r\nThe second optimization is to change the memory access patterns to get the II of the inner loops down to 1, which means the entire design has a throughput of about one set of 32 weights per clock cycle, which is the target we are aiming for. The previous design packed the output of a layer as follows:\r\n\r\n    u32 mask = 0x1 << shift;\r\n    scratch[1][j/32] = (scratch[1][j/32] & ~mask) | (out << shift);\r\n\r\nThe second line is a read-modify-write operation on scratch memory residing in block RAM, which will always take the hardware 2 cycles to complete. As a results, the maximum II of the inner loop is limited to 2. I change this code to the following:\r\n\r\n    tmp = tmp | (out << shift);\r\n    if (j % 32 == 31) {\r\n        scratch[1][j/32] = tmp;\r\n        tmp = 0;\r\n    }\r\n\r\nThe bits are now packed into a temporary register, which is then written to memory and cleared when it fills up. There is now only a memory write, and the II of the new design drops to 1 as expected. This new version now takes ~357,000 cycles to process one image.\r\n\r\nThe third optimization is to pipeline the outer loop (over output neurons). In HLS, pipelining outer loops creates significantly more hardware replication, but can give higher throughputs. In my case, the hardware for the entire inner loop is replicated multiple times. One interesting thing that HLS does is create 2 memory ports on my design (there are multiple physical ports connecting the FPGA logic to the memory bus). The loop hardware is replicated such that there is actually more than 1 read per clock cycle, and multiple sets of weights are processed in parallel in addition to the pipelining. This final version takes ~162,000 cycles to process one image, but takes up 87% of the LUTs on my FPGA board and likely is not realizable in the actual hardware.\r\n\r\n### Results\r\n\r\n(insert graph here)\r\n\r\nAlthough I did not have time to get my design to run on actual hardware, I will now give calculations for theoretical speedup and power efficiency. Deep learning tasks are known to be memory bandwidth bound [3], especially when the arithmetic intensity is low due to lack of convolutional layers, which is the case in my project. My target platform is a Zybo Zynq development board, which has a memory bandwidth of 1050 Mbps. Assuming 90% saturation of memory bandwidth, or 118.125 MB/s, even the Optimization #1 version of my implementation becomes restricted by the memory bandwidth. There are about 1.3 MB of weights that need to be read for processing each image, which gives a theoretical result of 0.011s per input image on the FPGA hardware.\r\n\r\nI measured CPU and GPU execution time of the same neural network architecture that I implemented, the only difference being that the CPU and GPU do not pack the binary weights densely. Both CPU and GPU implementations use off-the-shelf Torch modules with no additional optimizations. A i7-3770K CPU takes on average 0.0113s per image, and a GTX980 GPU takes 0.0000825s per image. The i7-3770K has a TDP of 77W, and the GTX980 165W. I used Xilinx's Power Estimator tool and got a power usage of 1.9W for my Optimization #1 design. All of these numbers give the following graph for energy used per image:\r\n\r\n(insert graph)\r\n\r\nMy low-end FPGA development board is much more efficient than a CPU neural network implementation, and barely worse off than the GPU. The GPU beats the FPGA mainly due to its massive memory bandwidth of 224 GB/s, which lets it excel at bandwidth-bound tasks. In terms of total power consumption, the FPGA is by far the lowest with 1.9W, suitable for even mobile applications. These results indicate that an FPGA implementation of bitwise neural networks excels in situations that demand both decent performance and low total power consumption.\r\n\r\n### References:\r\n[1] Courbariaux, Matthieu, Hubara, Itay, Soudry, Daniel, El-Yaniv, Ran, and Bengio, Yoshua. Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1. http://arxiv.org/pdf/1602.02830.pdf\r\n\r\n[2] Kim, Minje and Smaragdis, Paris. Bitwise Neural Networks. http://arxiv.org/pdf/1601.06071v1.pdf\r\n\r\n[3] Deng, Zhaoxia, Xu, Cong, Cai, Qiong, and Faraboschi, Paolo. Reduced-Precision Memory Value Approximation for Deep Learning. http://www.labs.hpe.com/techreports/2015/HPL-2015-100.pdf\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}