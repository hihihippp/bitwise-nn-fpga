<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Bitwise-nn-fpga : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Bitwise-nn-fpga</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/tdrussell/bitwise-nn-fpga">View on GitHub</a>

          <h1 id="project_title">Bitwise-nn-fpga</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/tdrussell/bitwise-nn-fpga/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/tdrussell/bitwise-nn-fpga/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="fpga-implementation-of-bitwise-neural-networks" class="anchor" href="#fpga-implementation-of-bitwise-neural-networks" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>FPGA Implementation of Bitwise Neural Networks</h1>

<p>Trace Russell
Carnegie Mellon University, 15418 Spring 2016 Final Project</p>

<h2>
<a id="final-writeup" class="anchor" href="#final-writeup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Writeup</h2>

<p>I developed an FPGA implementation of bitwise neural networks, where all of the arithmetic operations consist solely of bitwise operations and integer addition/comparison. I explore several different optimizations for adding pipelining and parallelism to the hardware to get speedups over the baseline sequential implementation. I did not have time to get my implementation running on actual hardware, but I give theoretical calculations showing how my implementation would get several orders of magnitude speedup over off-the-shelf non-bitwise neural network implementations for CPUs and GPUs, primarily due to more efficient use of memory bandwidth.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background:</h3>

<p>A bitwise neural network is a neural network where all the weights and activations are binary values. A forward pass through the network consists only of bitwise operations, plus a few integer additions and comparisons. As a result, it is possible to efficiently implement these types of networks on an FPGA, because there is no need for expensive multiply operations.</p>

<p>For a fully-connected neural network for classification tasks, the input is a binary vector, and the output is an N-dimensional vector, where N is the number of classes. Each non-output layer of the network consists of "multiplying" the input vector by a weight matrix (the binary multiplication is actually just XNOR), and then comparing the integer values of the output vector with a threshold to convert them back into binary values. The output layer of the network skips the thresholding step, and instead the class with the largest integer value is the predicted class. The basics of computing a forward pass through the network can be summarized as follows:</p>

<pre><code>for each layer:
    for each output neuron:
        acc = 0
        for each input neuron:
            acc += weight XNOR input
        output = threshold(acc)
</code></pre>

<p>The outermost 'layer' loop is not parallelizable, because each layer depends on the results of the previous layer. However, either of the innermost loops is trivially parallelizable. Because there is only about 1 mathematical operation per weight, it turns out that memory bandwidth becomes the limiting factor for getting speedup improvements. As a result, for a hardware implementation most of the speedup is achieved through pipelining the innermost loops. I discuss this in more detail later, but a key takeaway is that most of the parallelism in a hardware bitwise neural network implementation comes in the form of pipelining. The goal is to achieve a total throughput of 1 operation per cycle, as this will saturate the memory bus and any further speedups are impossible.</p>

<h3>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>

<p>I first started by using open source bitwise neural network code [1] to train a network from scratch. I made some modifications to the code, such as removing a few non-bitwise operations from the network and simplifying batch normalization layers. I then wrote some Lua scripts (available in this repository) to extract the binary weights and inputs, and package them in a format suitable for loading from C code. I used the MNIST handwritten digits dataset, and the network architecture consists of 4 fully-connected layers. The sizes of the layers are 784-2048-2048-2048-10. The trained bitwise neural network achieves a test set accuracy of about 94%.</p>

<p>I implemented the main computational kernel using Xilinx's Vivado High-Level Synthesis (HLS), which which takes a sequential C or C++ description of an algorithm and synthesizes it into RTL Verilog or VHDL. HLS automatically parallelizes any operation it can. A good example of this is the following code:</p>

<p>(insert code here)</p>

<p>A naive hardware synthesis of this code would take 32 cycles to complete. However, HLS is able to automatically infer a combinatorial logarithmic adder tree implementation, which only takes 1 cycle to complete. In addition to automatic parallelization, HLS lets the programmer specify various synthesis directives such as array partitioning and loop pipelining to guide the synthesis process and expose more parallelism.</p>

<p>My baseline implementation is a sequential version of the algorithm with no pipelining. The only parallelism present is what HLS is able to infer without any added synthesis directives. This version takes ~11,000,000 cycles to process one input image and predict a classification. The design uses 11% of the lookup tables (LUTs) for my FPGA board.</p>

<p>The first optimization I make is to add pipelining to the inner loop (over input neurons). The new version takes ~627,000 cycles to process one image. The non-output layers have an initiation interval (II) of 2, which means they are pipelined and start processing a new set of 32 weights every 2 clock cycles. This design uses 16% of the LUTs, a moderate increases over the sequential version due to the pipelining, which must replicate hardware.</p>

<p>The second optimization is to change the memory access patterns to get the II of the inner loops down to 1, which means the entire design has a throughput of about one set of 32 weights per clock cycle, which is the target we are aiming for. The previous design packed the output of a layer as follows:</p>

<pre><code>u32 mask = 0x1 &lt;&lt; shift;
scratch[1][j/32] = (scratch[1][j/32] &amp; ~mask) | (out &lt;&lt; shift);
</code></pre>

<p>The second line is a read-modify-write operation on scratch memory residing in block RAM, which will always take the hardware 2 cycles to complete. As a results, the maximum II of the inner loop is limited to 2. I change this code to the following:</p>

<pre><code>tmp = tmp | (out &lt;&lt; shift);
if (j % 32 == 31) {
    scratch[1][j/32] = tmp;
    tmp = 0;
}
</code></pre>

<p>The bits are now packed into a temporary register, which is then written to memory and cleared when it fills up. There is now only a memory write, and the II of the new design drops to 1 as expected. This new version now takes ~357,000 cycles to process one image.</p>

<p>The third optimization is to pipeline the outer loop (over output neurons). In HLS, pipelining outer loops creates significantly more hardware replication, but can give higher throughputs. In my case, the hardware for the entire inner loop is replicated multiple times. One interesting thing that HLS does is create 2 memory ports on my design (there are multiple physical ports connecting the FPGA logic to the memory bus). The loop hardware is replicated such that there is actually more than 1 read per clock cycle, and multiple sets of weights are processed in parallel in addition to the pipelining. This final version takes ~162,000 cycles to process one image, but takes up 87% of the LUTs on my FPGA board and likely is not realizable in the actual hardware.</p>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References:</h3>

<p>[1] Binarized Neural Networks: Training Neural Networks with Weights and
Activations Constrained to +1 or -1. (<a href="http://arxiv.org/pdf/1602.02830.pdf">http://arxiv.org/pdf/1602.02830.pdf</a>)</p>

<p>[2] Bitwise Neural Networks. (<a href="http://arxiv.org/pdf/1601.06071v1.pdf">http://arxiv.org/pdf/1601.06071v1.pdf</a>)</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bitwise-nn-fpga maintained by <a href="https://github.com/tdrussell">tdrussell</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
